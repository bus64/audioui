Source Directory: maestro

# File: src/core/audio/maestro/arrangement_engine.py © 2025 projectemergence. All rights reserved.

import random
from typing import Dict, List, Tuple
from core.audio.maestro.harmonic      import HarmonicAnalyser
from core.audio.maestro.progression   import ProgressionSynth
from core.audio.maestro.orchestrator  import Orchestrator
from core.audio.maestro.automix       import AutoMixer

class ArrangementEngine:
    def __init__(self, client):
        self.client = client
        self.ha      = HarmonicAnalyser()
        self.ps      = ProgressionSynth()
        self.orc     = Orchestrator()
        self.mix     = AutoMixer()

    def prepare_block(self, beats: float = 4.0) -> Dict[str, Dict]:
        """
        1) Pull melodic events from the Compositor until `beats` are filled.
        2) Analyze melody → chord progression → orchestration.
        3) Inject the raw melody as its own part.
        4) Auto-mix & return the per-preset configs.
        """
        # 1) Gather melody events
        melody_notes, melody_durs, melody_ints = [], [], []
        time_acc = 0.0
        while time_acc < beats:
            notes, durs, ints = self.client.maestro.compositor.next_event()
            for n, d, i in zip(notes, durs, ints):
                melody_notes.append(n)
                melody_durs.append(d)
                melody_ints.append(i)
                time_acc += d

        # 2) Harmonic analysis & chord progression
        analysis = self.ha.describe(list(zip(melody_notes, melody_durs)))
        chords   = self.ps.next(analysis, beats)
        chord_dur = beats / max(len(chords), 1)
        chord_durs = [chord_dur] * len(chords)

        # 3) Orchestrate the chords (e.g. bass, piano, etc.)
        parts = self.orc.voice(chords, chord_durs)

        # 4) Inject the raw melody line
        parts['melody'] = {
            'notes':     melody_notes,
            'durations': melody_durs,
            'intensity': melody_ints
        }

        # 5) Auto-mix levels & return
        return self.mix.autoset(parts)

    def render_block(self, parts_fx: Dict[str, Dict]) -> None:
        for preset, cfg in parts_fx.items():
            self.client.play_preset(preset, **cfg)

    # … keep queue_next_melody, _note_exchange, etc. as before …


    def queue_next_melody(self, melody, blocks_delay=2):
        self.next_melody = melody
        self.transition_blocks = blocks_delay

    def _note_exchange(self, parts):
        prob = 1 - self.transition_blocks / max(self.transition_blocks,1)
        for cfg in parts.values():
            for i, n in enumerate(cfg['notes']):
                if random.random() < prob and i < len(self.next_melody):
                    cfg['notes'][i] = self.next_melody[i][0]
        self.transition_blocks -= 1
        if self.transition_blocks <= 0:
            self.current_melody = self.next_melody
            self.next_melody = None
        return parts


# File: src/core/audio/maestro/audio_maestro.py  © 2025 projectemergence. All rights reserved.

import asyncio
import inspect
import logging
import math
import os
import pkgutil
import random
import time
from concurrent.futures import Future
from typing import Any, Dict, Set

from core.audio.presets.base_preset import BasePreset
from core.audio.maestro.maestro_compositor import Compositor
from core.audio.maestro.arrangement_engine import ArrangementEngine

class ParamMeta:
    """Helper for random-stepping numeric or boolean preset parameters."""
    def __init__(self, default: Any):
        self.default = default
        self.is_list = isinstance(default, list)
        self.is_bool = isinstance(default, bool)
        self.is_int  = isinstance(default, int) and not self.is_bool
        self.is_float= isinstance(default, float)

        if self.is_int or self.is_float:
            val = float(default)
            lo  = val * 0.5 if val > 0 else 0.0
            hi  = val * 2.0 if val > 0 else 1.0
            if self.is_int:
                self.lo, self.hi = int(lo), int(hi)
                self.val = int(default)
            else:
                self.lo, self.hi = lo, hi
                self.val = float(default)
        else:
            self.val = default

        # only non-list params get random stepping
        self.step_chance = 0.0 if self.is_list else 1.0

    def step(self) -> Any:
        if random.random() > self.step_chance:
            return self.val
        if self.is_int:
            nv = self.val + random.choice([-1, 1])
            self.val = max(self.lo, min(self.hi, nv))
            return self.val
        if self.is_float:
            step = abs(self.val) * 0.1 if abs(self.val) > 1e-6 else 0.1
            nv = self.val + random.uniform(-step, step)
            self.val = max(self.lo, min(self.hi, nv))
            return round(self.val, 3)
        if self.is_bool:
            self.val = not self.val
            return self.val
        return self.val

class Maestro:
    """
    Async Generative Music Maestro:
      • Block-level scheduling via the client’s asyncio loop
      • Zone control: set_zone / leave_zone
      • Random-stepping of static preset params
    """
    def __init__(self, client):
        self.client = client
        self.logger = logging.getLogger(__name__)

        # zone → presets; zone → Future
        self.zones: Dict[str, Set[str]]         = {}
        self.tasks: Dict[str, Future]          = {}

        # global LFO state
        self.tempo     = 180.0
        self.energy    = 0.7
        self._last_time= time.time()
        self._phase    = 0.2

        # parameter introspection
        self.presets_sig: Dict[str, inspect.Signature]     = {}
        self.presets_meta: Dict[str, Dict[str, ParamMeta]] = {}
        self._introspect_presets()

        # melody source & arranger
        melodies_path = os.path.abspath(
            os.path.join(os.path.dirname(__file__), "..", "melodies")
        )
        self.compositor = Compositor(melodies_path)
        self.arranger   = ArrangementEngine(self.client)
        
    def enter_zone(self, zone: str, presets: Set[str]):
        """
        Start or update block loop for this zone.
        Cancels any existing loop for that zone, then schedules a new
        _zone_block_loop on the client’s asyncio event loop.
        """
        # cancel existing
        existing = self.tasks.pop(zone, None)
        if existing:
            existing.cancel()

        self.zones[zone] = presets

        # schedule the async zone loop on the client's loop
        fut = asyncio.run_coroutine_threadsafe(
            self._zone_block_loop(zone),
            self.client.loop
        )
        self.tasks[zone] = fut

    set_zone = enter_zone
    def leave_zone(self, zone: str):
        """
        Stop and remove a zone.
        """
        fut = self.tasks.pop(zone, None)
        if fut:
            fut.cancel()
        self.zones.pop(zone, None)

    async def _zone_block_loop(self, zone: str):
        """
        Loop per block:
          1) update LFOs
          2) step & play static presets
          3) run arranger pipeline for melodic presets
          4) sleep for block duration
        """
        block_beats = 8.0
        while True:
            try:
                # 1) update globals
                self._update_tempo()
                self._update_energy()

                presets = self.zones.get(zone, set())
                static, has_melodic = [], False

                # separate static vs melodic
                for p in presets:
                    sig = self.presets_sig.get(p)
                    if not sig:
                        static.append(p)
                        continue
                    req = {
                        n for n, param in sig.parameters.items()
                        if n != "self" and param.default is inspect._empty
                    }
                    if {"notes", "durations"}.issubset(req):
                        has_melodic = True
                    else:
                        static.append(p)

                # 2) play static presets
                for p in static:
                    meta = self.presets_meta.get(p, {})
                    params = {n: pm.step() for n, pm in meta.items()}
                    # schedule play on the same audio client
                    asyncio.run_coroutine_threadsafe(
                        self.client.cmd_queue.put({
                            "cmd": "play_preset",
                            "preset": p,
                            "params": params
                        }),
                        self.client.loop
                    )

                # 3) melodic arranger
                if has_melodic:
                    parts = self.arranger.prepare_block(beats=block_beats)
                    self.arranger.render_block(parts)

                # 4) wait
                seconds = block_beats * (60.0 / self.tempo)
                await asyncio.sleep(seconds)

            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.warning(f"Error in zone loop '{zone}': {e}")

    def _introspect_presets(self):
        import core.audio.presets as pkg
        for it in pkgutil.iter_modules(pkg.__path__):
            try:
                mod = __import__(f"{pkg.__name__}.{it.name}", fromlist=["*"])
                for _, cls in inspect.getmembers(mod, inspect.isclass):
                    if issubclass(cls, BasePreset) and cls is not BasePreset:
                        sig = inspect.signature(cls.__init__)
                        self.presets_sig[it.name] = sig
                        meta = {}
                        for n, param in sig.parameters.items():
                            if n != "self" and param.default is not inspect._empty:
                                meta[n] = ParamMeta(param.default)
                        self.presets_meta[it.name] = meta
            except Exception:
                continue

    def _update_tempo(self):
        step = self.tempo * random.uniform(-0.07, 0.07)
        self.tempo = max(60.0, min(240.0, self.tempo + random.uniform(-step, step)))

    def _update_energy(self):
        now = time.time()
        dt = now - self._last_time
        beat_time = 60.0 / self.tempo
        self._phase = (self._phase + dt / (32 * beat_time)) % 1.0
        self.energy = 0.7 + 0.3 * math.sin(2 * math.pi * self._phase)
        self._last_time = now


#File:  audio/automix.py © 2025 projectemergence. All rights reserved.
# automix.py
import pyloudnorm as pyln, numpy as np
from pydub import AudioSegment, effects

class AutoMixer:
    def __init__(self, target_lufs: float = -14):
        self.meter = pyln.Meter(48000)
        self.target = target_lufs

    def autoset(self, parts: dict, target_lufs: float | None = None):
        target = target_lufs or self.target
        for cfg in parts.values():
            seg = self._sine_stub(cfg['notes'], cfg['durations'])
            loud = self.meter.integrated_loudness(seg.get_array_of_samples())
            gain = target - loud
            cfg['gain_db'] = gain
            # simple fx heuristics
            cfg['enable_reverb'] = np.mean(cfg['notes']) > 60
            cfg['enable_chorus'] = len(cfg['notes']) > 6
        return parts

    @staticmethod
    def _sine_stub(notes, durs, sr=48000):
        samples = np.concatenate([np.sin(2*np.pi*440*2**((n-69)/12)*
                            np.linspace(0,d,int(sr*d),False)) for n,d in zip(notes,durs)])
        return AudioSegment(
            (samples*32767).astype(np.int16).tobytes(),
            frame_rate=sr, sample_width=2, channels=1
        )


#File:  audio/harmonic.py © 2025 projectemergence. All rights reserved.
# src/core/audio/harmonic.py

import math
from typing import List, Tuple, Dict

import numpy as np
from music21 import note, stream, analysis

# build major/minor triad templates in pitch‐class space
_NOTE_NAMES = ['C','C#','D','D#','E','F','F#','G','G#','A','A#','B']
_CHORD_TEMPLATES: Dict[str, List[int]] = {}
for i, name in enumerate(_NOTE_NAMES):
    _CHORD_TEMPLATES[f"{name}"]  = [i, (i+4)%12, (i+7)%12]   # major
    _CHORD_TEMPLATES[f"{name}m"] = [i, (i+3)%12, (i+7)%12]   # minor


class HarmonicAnalyser:
    """
    Real‐time key, chord & function estimation without external ML:
      • key via music21.Krumhansl
      • chords via 1‐beat triad‐template matching
      • functions via scale‐degree rule (I/vi=T, ii/IV=S, V/vii°=D)
    """

    def describe(
        self,
        melody: List[Tuple[float, float]]
    ) -> Dict[str, List]:
        """
        Input:
          melody = [(frequency_Hz, duration_beats), …]
        Returns:
          {
            "key":       "C major",
            "chords":    [...str chord symbol per beat…],
            "functions": [...int 0/1/2 per beat…],
            "durations": [...float, all 1.0…]
          }
        """
        # 1) Key estimation
        s = stream.Stream()
        offset = 0.0
        for freq, dur in melody:
            n = note.Note()
            n.pitch.frequency = freq
            n.offset = offset
            n.quarterLength = dur
            s.insert(n)
            offset += dur
        key_obj = s.analyze("Krumhansl")
        key = f"{key_obj.tonic.name} {key_obj.mode}"

        # 2) collect events as (time, pitch_class)
        events: List[Tuple[float,int]] = []
        t = 0.0
        for freq, dur in melody:
            midi = int(round(69 + 12*math.log2(freq/440.0)))
            pc = midi % 12
            events.append((t, pc))
            t += dur

        total_beats = math.ceil(offset)
        chords:    List[str]   = []
        durations: List[float] = []

        tonic_pc = _NOTE_NAMES.index(key_obj.tonic.name)

        # 3) for each beat‐window pick best‐matching triad
        for b in range(total_beats):
            window_pcs = [pc for time, pc in events if b <= time < b+1]
            if not window_pcs:
                # no notes → default to I
                symbol = key_obj.tonic.name + ("" if key_obj.mode=="major" else "m")
            else:
                hist = np.zeros(12, dtype=int)
                for pc in window_pcs:
                    hist[pc] += 1
                best_score, symbol = -1, key_obj.tonic.name
                for sym, template in _CHORD_TEMPLATES.items():
                    score = int(sum(hist[pc] for pc in template))
                    if score > best_score:
                        best_score, symbol = score, sym
            chords.append(symbol)
            durations.append(1.0)

        # 4) map each chord → function (0=T,1=S,2=D)
        functions: List[int] = []
        for sym in chords:
            root = sym.rstrip('m')
            root_pc = _NOTE_NAMES.index(root) if root in _NOTE_NAMES else tonic_pc
            interval = (root_pc - tonic_pc) % 12
            if interval in (7, 11):      # V or vii°
                functions.append(2)
            elif interval in (2, 5):     # ii or IV
                functions.append(1)
            else:                        # I, vi, III, etc.
                functions.append(0)

        return {
            "key":       key,
            "chords":    chords,
            "functions": functions,
            "durations": durations
        }


# File: src/core/audio/maestro_compositor.py © 2025 projectemergence. All rights reserved.

import os
import re
import json
import random
from typing import List, Tuple, Dict, Optional

# A single note event: (frequency in Hz, duration in beats, intensity 0–1)
NoteEvent = Tuple[float, float, float]

class Compositor:
    """
    • Loads JSON5‐style melody files (comments allowed)
    • Captures all metadata (title, structure, remix params, etc.)
    • Falls back to legacy "notes" arrays when "hands" is absent
    • Exposes get_full_sequence() and next_event() as before
    """
    def __init__(self, repo_path: str, default_intensity: float = 0.8):
        self.repo_path         = repo_path
        self.default_intensity = default_intensity

        # name → metadata dict (everything except 'hands'/'notes')
        self.metadata:  Dict[str, Dict]              = {}
        # name → list of hands → list of NoteEvent
        self.melodies:  Dict[str, List[List[NoteEvent]]] = {}
        # name → tempo (BPM)
        self.tempos:    Dict[str, float]              = {}
        # name → (beats_per_bar, beat_unit)
        self.meters:    Dict[str, Tuple[int,int]]     = {}

        # playback state
        self.current_hands:   List[List[NoteEvent]] = []
        self.idxs:            List[int]             = []
        self.current_melody:  Optional[str]         = None

        print(f"[Compositor] Scanning melodies in {self.repo_path}")
        self._load_repo()

    def _load_repo(self):
        def strip_comments(text: str) -> str:
            # remove /* ... */ and //... comments
            without_block = re.sub(r'/\*.*?\*/', '', text, flags=re.DOTALL)
            without_line  = re.sub(r'//.*', '', without_block)
            return without_line

        for fn in sorted(os.listdir(self.repo_path)):
            if not fn.lower().endswith(".json"):
                continue
            path = os.path.join(self.repo_path, fn)
            name = fn[:-5]
            try:
                raw  = open(path, encoding="utf-8").read()
                data = json.loads(strip_comments(raw))
            except Exception as e:
                print(f"[Compositor] Failed to parse {fn}: {e}")
                continue

            # 1) store metadata (all keys except 'hands' and 'notes')
            md = {k: v for k, v in data.items() if k not in ("hands", "notes")}
            self.metadata[name] = md

            # 2) read tempo & meter
            if "tempo" in data:
                self.tempos[name] = float(data["tempo"])
                print(f"[Compositor] '{name}': tempo={data['tempo']} BPM")
            if "time_signature" in data:
                tsig = data["time_signature"]
                try:
                    num, den = map(int, tsig.split("/"))
                    self.meters[name] = (num, den)
                    print(f"[Compositor] '{name}': time_signature={tsig}")
                except:
                    print(f"[Compositor] '{name}': invalid time_signature '{tsig}'")

            # 3) gather raw hands or fallback to legacy notes
            hands_raw = data.get("hands")
            if hands_raw is None and "notes" in data:
                # legacy single-hand format
                hands_raw = [data["notes"]]

            hands_evs: List[List[NoteEvent]] = []
            if isinstance(hands_raw, list) and all(isinstance(h, list) for h in hands_raw):
                for hl in hands_raw:
                    evs: List[NoteEvent] = []
                    for o in hl:
                        try:
                            f = float(o["frequency"])
                            d = float(o.get("duration_beats", o.get("duration", 1.0)))
                            i = float(o.get("intensity", self.default_intensity))
                            evs.append((f, d, i))
                        except:
                            continue
                    if evs:
                        hands_evs.append(evs)

            # 4) register or skip
            if hands_evs:
                self.melodies[name] = hands_evs
                print(f"[Compositor] Registered '{name}' with {len(hands_evs)} hand(s)")
            else:
                print(f"[Compositor] No valid events in '{name}', skipping")

    def start(self, melody_name: str):
        """Begin fresh run through up to 4 hands."""
        self.current_melody = melody_name
        self.current_hands  = self.melodies.get(melody_name, [])
        self.idxs           = [0] * len(self.current_hands)
        print(f"[Compositor] Starting '{melody_name}' with {len(self.current_hands)} hand(s)")

    def next_event(self) -> Tuple[List[float], List[float], List[float]]:
        """Return parallel lists: notes, durations, intensities."""
        if not self.current_hands:
            return [0.0], [1.0], [0.0]

        notes, durs, ints = [], [], []
        for hi, hand in enumerate(self.current_hands):
            f, d, i = hand[self.idxs[hi]]
            notes.append(f)
            durs.append(d)
            ints.append(i)
            self.idxs[hi] = (self.idxs[hi] + 1) % len(hand)

        print(f"[Compositor] next_event → notes={notes}, durs={durs}, ints={ints}")
        return notes, durs, ints

    def get_full_sequence(self) -> Tuple[List[float], List[float], List[float]]:
        """
        Return the entire upcoming sequence of notes, durations and intensities
        for the *current* melody’s first hand.
        """
        if not self.current_hands:
            return [], [], []
        evs   = self.current_hands[0]
        notes = [f for (f, _, _) in evs]
        durs  = [d for (_, d, _) in evs]
        ints  = [i for (_, _, i) in evs]
        return notes, durs, ints

    def sprinkle(self, chance: float = 0.1) -> bool:
        hit = random.random() < chance
        if hit:
            print(f"[Compositor] sprinkle fired (chance={chance})")
        return hit

    def get_tempo(self, default: float) -> float:
        """Return the stored tempo for the current melody, or default."""
        return self.tempos.get(self.current_melody, default)

    def get_meter(self) -> Tuple[int,int]:
        """Return (beats_per_bar, beat_unit) for current melody, default (4,4)."""
        return self.meters.get(self.current_melody, (4,4))


#File:  audio/maestro_mixer.py © 2025 projectemergence. All rights reserved.
import numpy as np
from collections import deque

class Mixer:
    """
    Perform real-time spectral analysis and drive a parametric EQ.
    """
    def __init__(self, client, sample_rate=48000, frame_size=1024):
        self.client       = client
        self.sr           = sample_rate
        self.N            = frame_size
        # rolling history of spectra for smoothing
        self.history      = deque(maxlen=4)
        # EQ band centers (Hz)
        self.bands        = [125, 250, 500, 1000, 2000, 4000, 8000, 16000]
        # initial gains
        self.band_gains   = {b: 1.0 for b in self.bands}

    def analyze_and_eq(self, audio_frame: np.ndarray):
        """
        audio_frame: 1-D float32 buffer of length N.
        """
        # 1) compute magnitude spectrum
        spec = np.abs(np.fft.rfft(audio_frame * np.hanning(self.N)))
        self.history.append(spec)
        avg_spec = np.mean(self.history, axis=0)

        # 2) for each band, find corresponding bin range
        freqs = np.fft.rfftfreq(self.N, 1/self.sr)
        for center in self.bands:
            # find nearest bin
            idx = np.argmin(np.abs(freqs - center))
            magnitude = avg_spec[idx]
            # map magnitude → desired gain (simple inverse)
            # clamp between 0.5 and 1.0
            gain = float(max(0.5, min(1.0, 1.0 - (magnitude / avg_spec.max())*0.5)))
            # smooth update (one-pole)
            self.band_gains[center] = 0.8*self.band_gains[center] + 0.2*gain
            # push to client’s EQ
            self.client.set_eq_gain(center, self.band_gains[center])


#File:  audio/orchestrator.py © 2025 projectemergence. All rights reserved.
# orchestrator.py
from music21 import chord, instrument

REGISTER = {
    'bass': (28, 48),      # E1–C3
    'piano': (50, 96),     # D3–C7
    'pad': (40, 84),
    'lead': (60, 108),
}

class Orchestrator:
    def __init__(self):
        self.occ = {name: 0 for name in REGISTER}

    def voice(self, chords: list[str], rhythm: list[float]):
        parts = {}
        for symb, dur in zip(chords, rhythm):
            c = chord.Chord(symb)
            bass_note = c.bass().pitch.midi
            # allocate bass
            parts.setdefault('bass', {'notes':[], 'durations':[], 'intensity':[]})
            parts['bass']['notes'].append(self._fit(bass_note, 'bass'))
            parts['bass']['durations'].append(dur)
            parts['bass']['intensity'].append(.9)

            # allocate chord tones to piano spread over two octaves
            p_notes = [self._fit(n.pitch.midi, 'piano') for n in c.notes]
            parts.setdefault('piano', {'notes':[], 'durations':[], 'intensity':[]})
            parts['piano']['notes'].extend(p_notes)
            parts['piano']['durations'].extend([dur/len(p_notes)]*len(p_notes))
            parts['piano']['intensity'].extend([.7]*len(p_notes))
        return parts

    def _fit(self, midi, role):
        low, high = REGISTER[role]
        while midi < low:  midi += 12
        while midi > high: midi -= 12
        self.occ[role] += 1
        return midi


# File: src/core/audio/progression.py © 2025 projectemergence. All rights reserved.

from typing import Dict, Any, List
import random

from music21 import key as m21key, roman

class ProgressionSynth:
    """
    Purely rule-based chord progression generator.
    • Supports genre-specific templates (pop, rock, blues, jazz, classical, funk)
    • Falls back to a functional-harmony Markov chain if needed
    • Outputs real chord symbols in the current key (e.g. 'C', 'Am', 'F')
    """

    # genre → Roman-numeral template per bar
    GENRE_TEMPLATES: Dict[str, List[str]] = {
        'pop':       ['I', 'V', 'vi', 'IV'],
        'rock':      ['I', 'IV', 'V'],
        'blues':     ['I', 'IV', 'I', 'V'],
        'jazz':      ['ii', 'V', 'I'],
        'classical': ['I', 'vi', 'ii', 'V'],
        'funk':      ['I', 'bVII', 'IV', 'I'],
    }

    # Markov-chain transitions between functions: T→S→D→T
    FUNCTION_MARKOV: Dict[str, List[str]] = {
        'T': ['S']*3 + ['D']*2 + ['T'],   # tonic tends to go to subdominant
        'S': ['D']*4 + ['T'],             # subdominant to dominant
        'D': ['T']*5 + ['S'],             # dominant resolves to tonic
    }

    # map Roman-numeral to its function
    RN_TO_FUNCTION: Dict[str, str] = {
        'I': 'T','i':'T','vi':'T','VI':'T','III':'T','iii':'T',
        'ii':'S','II':'S','IV':'S','iv':'S',
        'V':'D','v':'D','vii°':'D','VII':'D'
    }

    def __init__(self, genre: str = 'pop', temperature: float = 0.5):
        """
        :param genre: one of GENRE_TEMPLATES keys; if unknown, uses Markov fallback
        :param temperature: controls randomness in Markov chain (0–1)
        """
        self.genre = genre if genre in self.GENRE_TEMPLATES else None
        self.temperature = max(0.0, min(1.0, temperature))

    def next(self, analysis: Dict[str,Any], beats: float) -> List[str]:
        """
        :param analysis: must include 'key' (e.g. "C major") and optionally 'durations'
        :param beats: total number of beats to fill
        :returns: list of chord symbols (e.g. ['C','G','Am','F',…])
        """
        # parse the key
        key_str = analysis.get('key', 'C major')
        key_str = key_str.translate(str.maketrans({'♭':'b','♯':'#'}))
        m21k = m21key.Key(key_str)

        # estimate bars to fill (assume 4/4 unless analysis provides meter)
        bar_beats = 4.0
        # if 'time_signature' in analysis, override bar_beats
        if 'time_signature' in analysis:
            num, den = map(int, analysis['time_signature'].split('/'))
            bar_beats = num * (4/den)
        n_bars = max(1, int(round(beats / bar_beats)))

        # choose progression template or fallback
        if self.genre:
            template = self.GENRE_TEMPLATES[self.genre]
            # repeat/truncate to match n_bars
            r = (template * ((n_bars // len(template)) + 1))[:n_bars]
            rn_sequence = r
        else:
            # functional Markov
            rn_sequence = []
            prev_fn = 'T'
            for _ in range(n_bars):
                fn_choices = self.FUNCTION_MARKOV[prev_fn]
                fn = random.choices(fn_choices, k=1)[0]
                # pick a random Roman numeral with that function
                cands = [rn for rn,f in self.RN_TO_FUNCTION.items() if f==fn]
                rn = random.choice(cands)
                rn_sequence.append(rn)
                prev_fn = fn

        # convert Roman numerals → concrete chord symbols
        chords: List[str] = []
        for rn in rn_sequence:
            try:
                rn_obj = roman.RomanNumeral(rn, m21k)
                root = rn_obj.root().name  # e.g. 'C', 'A'
                quality = rn_obj.quality  # 'major', 'minor', 'dominant', etc.
            except Exception:
                # fallback if music21 can't parse bVII etc.
                # interpret 'bVII' relative to m21k
                if rn.startswith('b'):
                    degree = m21k.getScale().getDegreeFromPitch(rn[1:]) - 1
                    root = m21k.pitchFromDegree(degree).name
                    quality = 'major'
                else:
                    root = m21k.tonic.name
                    quality = 'major'
            # build symbol
            if quality == 'minor':
                sym = f"{root}m"
            elif quality == 'dominant':
                sym = f"{root}7"
            else:
                sym = root
            chords.append(sym)

        return chords


#File:  src/core/audio/__init__.py © 2025 projectemergence. All rights reserved.
#File:  src/core/__init__.py © 2024 projectemergence. All rights reserved.
# This file can be left empty, or you can use it to perform package-level initialization if needed.


