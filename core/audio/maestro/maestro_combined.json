{
    "src_dir": "maestro",
    "files": [
        {
            "file_path": "arrangement_engine.py",
            "content": "# File: src/core/audio/maestro/arrangement_engine.py \u00a9 2025 projectemergence. All rights reserved.\n\nimport random\nfrom typing import Dict, List, Tuple\nfrom core.audio.maestro.harmonic      import HarmonicAnalyser\nfrom core.audio.maestro.progression   import ProgressionSynth\nfrom core.audio.maestro.orchestrator  import Orchestrator\nfrom core.audio.maestro.automix       import AutoMixer\n\nclass ArrangementEngine:\n    def __init__(self, client):\n        self.client = client\n        self.ha      = HarmonicAnalyser()\n        self.ps      = ProgressionSynth()\n        self.orc     = Orchestrator()\n        self.mix     = AutoMixer()\n\n    def prepare_block(self, beats: float = 4.0) -> Dict[str, Dict]:\n        \"\"\"\n        1) Pull melodic events from the Compositor until `beats` are filled.\n        2) Analyze melody \u2192 chord progression \u2192 orchestration.\n        3) Inject the raw melody as its own part.\n        4) Auto-mix & return the per-preset configs.\n        \"\"\"\n        # 1) Gather melody events\n        melody_notes, melody_durs, melody_ints = [], [], []\n        time_acc = 0.0\n        while time_acc < beats:\n            notes, durs, ints = self.client.maestro.compositor.next_event()\n            for n, d, i in zip(notes, durs, ints):\n                melody_notes.append(n)\n                melody_durs.append(d)\n                melody_ints.append(i)\n                time_acc += d\n\n        # 2) Harmonic analysis & chord progression\n        analysis = self.ha.describe(list(zip(melody_notes, melody_durs)))\n        chords   = self.ps.next(analysis, beats)\n        chord_dur = beats / max(len(chords), 1)\n        chord_durs = [chord_dur] * len(chords)\n\n        # 3) Orchestrate the chords (e.g. bass, piano, etc.)\n        parts = self.orc.voice(chords, chord_durs)\n\n        # 4) Inject the raw melody line\n        parts['melody'] = {\n            'notes':     melody_notes,\n            'durations': melody_durs,\n            'intensity': melody_ints\n        }\n\n        # 5) Auto-mix levels & return\n        return self.mix.autoset(parts)\n\n    def render_block(self, parts_fx: Dict[str, Dict]) -> None:\n        for preset, cfg in parts_fx.items():\n            self.client.play_preset(preset, **cfg)\n\n    # \u2026 keep queue_next_melody, _note_exchange, etc. as before \u2026\n\n\n    def queue_next_melody(self, melody, blocks_delay=2):\n        self.next_melody = melody\n        self.transition_blocks = blocks_delay\n\n    def _note_exchange(self, parts):\n        prob = 1 - self.transition_blocks / max(self.transition_blocks,1)\n        for cfg in parts.values():\n            for i, n in enumerate(cfg['notes']):\n                if random.random() < prob and i < len(self.next_melody):\n                    cfg['notes'][i] = self.next_melody[i][0]\n        self.transition_blocks -= 1\n        if self.transition_blocks <= 0:\n            self.current_melody = self.next_melody\n            self.next_melody = None\n        return parts\n"
        },
        {
            "file_path": "audio_maestro.py",
            "content": "# File: src/core/audio/maestro/audio_maestro.py  \u00a9 2025 projectemergence. All rights reserved.\n\nimport asyncio\nimport inspect\nimport logging\nimport math\nimport os\nimport pkgutil\nimport random\nimport time\nfrom concurrent.futures import Future\nfrom typing import Any, Dict, Set\n\nfrom core.audio.presets.base_preset import BasePreset\nfrom core.audio.maestro.maestro_compositor import Compositor\nfrom core.audio.maestro.arrangement_engine import ArrangementEngine\n\nclass ParamMeta:\n    \"\"\"Helper for random-stepping numeric or boolean preset parameters.\"\"\"\n    def __init__(self, default: Any):\n        self.default = default\n        self.is_list = isinstance(default, list)\n        self.is_bool = isinstance(default, bool)\n        self.is_int  = isinstance(default, int) and not self.is_bool\n        self.is_float= isinstance(default, float)\n\n        if self.is_int or self.is_float:\n            val = float(default)\n            lo  = val * 0.5 if val > 0 else 0.0\n            hi  = val * 2.0 if val > 0 else 1.0\n            if self.is_int:\n                self.lo, self.hi = int(lo), int(hi)\n                self.val = int(default)\n            else:\n                self.lo, self.hi = lo, hi\n                self.val = float(default)\n        else:\n            self.val = default\n\n        # only non-list params get random stepping\n        self.step_chance = 0.0 if self.is_list else 1.0\n\n    def step(self) -> Any:\n        if random.random() > self.step_chance:\n            return self.val\n        if self.is_int:\n            nv = self.val + random.choice([-1, 1])\n            self.val = max(self.lo, min(self.hi, nv))\n            return self.val\n        if self.is_float:\n            step = abs(self.val) * 0.1 if abs(self.val) > 1e-6 else 0.1\n            nv = self.val + random.uniform(-step, step)\n            self.val = max(self.lo, min(self.hi, nv))\n            return round(self.val, 3)\n        if self.is_bool:\n            self.val = not self.val\n            return self.val\n        return self.val\n\nclass Maestro:\n    \"\"\"\n    Async Generative Music Maestro:\n      \u2022 Block-level scheduling via the client\u2019s asyncio loop\n      \u2022 Zone control: set_zone / leave_zone\n      \u2022 Random-stepping of static preset params\n    \"\"\"\n    def __init__(self, client):\n        self.client = client\n        self.logger = logging.getLogger(__name__)\n\n        # zone \u2192 presets; zone \u2192 Future\n        self.zones: Dict[str, Set[str]]         = {}\n        self.tasks: Dict[str, Future]          = {}\n\n        # global LFO state\n        self.tempo     = 180.0\n        self.energy    = 0.7\n        self._last_time= time.time()\n        self._phase    = 0.2\n\n        # parameter introspection\n        self.presets_sig: Dict[str, inspect.Signature]     = {}\n        self.presets_meta: Dict[str, Dict[str, ParamMeta]] = {}\n        self._introspect_presets()\n\n        # melody source & arranger\n        melodies_path = os.path.abspath(\n            os.path.join(os.path.dirname(__file__), \"..\", \"melodies\")\n        )\n        self.compositor = Compositor(melodies_path)\n        self.arranger   = ArrangementEngine(self.client)\n        \n    def enter_zone(self, zone: str, presets: Set[str]):\n        \"\"\"\n        Start or update block loop for this zone.\n        Cancels any existing loop for that zone, then schedules a new\n        _zone_block_loop on the client\u2019s asyncio event loop.\n        \"\"\"\n        # cancel existing\n        existing = self.tasks.pop(zone, None)\n        if existing:\n            existing.cancel()\n\n        self.zones[zone] = presets\n\n        # schedule the async zone loop on the client's loop\n        fut = asyncio.run_coroutine_threadsafe(\n            self._zone_block_loop(zone),\n            self.client.loop\n        )\n        self.tasks[zone] = fut\n\n    set_zone = enter_zone\n    def leave_zone(self, zone: str):\n        \"\"\"\n        Stop and remove a zone.\n        \"\"\"\n        fut = self.tasks.pop(zone, None)\n        if fut:\n            fut.cancel()\n        self.zones.pop(zone, None)\n\n    async def _zone_block_loop(self, zone: str):\n        \"\"\"\n        Loop per block:\n          1) update LFOs\n          2) step & play static presets\n          3) run arranger pipeline for melodic presets\n          4) sleep for block duration\n        \"\"\"\n        block_beats = 8.0\n        while True:\n            try:\n                # 1) update globals\n                self._update_tempo()\n                self._update_energy()\n\n                presets = self.zones.get(zone, set())\n                static, has_melodic = [], False\n\n                # separate static vs melodic\n                for p in presets:\n                    sig = self.presets_sig.get(p)\n                    if not sig:\n                        static.append(p)\n                        continue\n                    req = {\n                        n for n, param in sig.parameters.items()\n                        if n != \"self\" and param.default is inspect._empty\n                    }\n                    if {\"notes\", \"durations\"}.issubset(req):\n                        has_melodic = True\n                    else:\n                        static.append(p)\n\n                # 2) play static presets\n                for p in static:\n                    meta = self.presets_meta.get(p, {})\n                    params = {n: pm.step() for n, pm in meta.items()}\n                    # schedule play on the same audio client\n                    asyncio.run_coroutine_threadsafe(\n                        self.client.cmd_queue.put({\n                            \"cmd\": \"play_preset\",\n                            \"preset\": p,\n                            \"params\": params\n                        }),\n                        self.client.loop\n                    )\n\n                # 3) melodic arranger\n                if has_melodic:\n                    parts = self.arranger.prepare_block(beats=block_beats)\n                    self.arranger.render_block(parts)\n\n                # 4) wait\n                seconds = block_beats * (60.0 / self.tempo)\n                await asyncio.sleep(seconds)\n\n            except asyncio.CancelledError:\n                break\n            except Exception as e:\n                self.logger.warning(f\"Error in zone loop '{zone}': {e}\")\n\n    def _introspect_presets(self):\n        import core.audio.presets as pkg\n        for it in pkgutil.iter_modules(pkg.__path__):\n            try:\n                mod = __import__(f\"{pkg.__name__}.{it.name}\", fromlist=[\"*\"])\n                for _, cls in inspect.getmembers(mod, inspect.isclass):\n                    if issubclass(cls, BasePreset) and cls is not BasePreset:\n                        sig = inspect.signature(cls.__init__)\n                        self.presets_sig[it.name] = sig\n                        meta = {}\n                        for n, param in sig.parameters.items():\n                            if n != \"self\" and param.default is not inspect._empty:\n                                meta[n] = ParamMeta(param.default)\n                        self.presets_meta[it.name] = meta\n            except Exception:\n                continue\n\n    def _update_tempo(self):\n        step = self.tempo * random.uniform(-0.07, 0.07)\n        self.tempo = max(60.0, min(240.0, self.tempo + random.uniform(-step, step)))\n\n    def _update_energy(self):\n        now = time.time()\n        dt = now - self._last_time\n        beat_time = 60.0 / self.tempo\n        self._phase = (self._phase + dt / (32 * beat_time)) % 1.0\n        self.energy = 0.7 + 0.3 * math.sin(2 * math.pi * self._phase)\n        self._last_time = now\n"
        },
        {
            "file_path": "automix.py",
            "content": "#File:  audio/automix.py \u00a9 2025 projectemergence. All rights reserved.\n# automix.py\nimport pyloudnorm as pyln, numpy as np\nfrom pydub import AudioSegment, effects\n\nclass AutoMixer:\n    def __init__(self, target_lufs: float = -14):\n        self.meter = pyln.Meter(48000)\n        self.target = target_lufs\n\n    def autoset(self, parts: dict, target_lufs: float | None = None):\n        target = target_lufs or self.target\n        for cfg in parts.values():\n            seg = self._sine_stub(cfg['notes'], cfg['durations'])\n            loud = self.meter.integrated_loudness(seg.get_array_of_samples())\n            gain = target - loud\n            cfg['gain_db'] = gain\n            # simple fx heuristics\n            cfg['enable_reverb'] = np.mean(cfg['notes']) > 60\n            cfg['enable_chorus'] = len(cfg['notes']) > 6\n        return parts\n\n    @staticmethod\n    def _sine_stub(notes, durs, sr=48000):\n        samples = np.concatenate([np.sin(2*np.pi*440*2**((n-69)/12)*\n                            np.linspace(0,d,int(sr*d),False)) for n,d in zip(notes,durs)])\n        return AudioSegment(\n            (samples*32767).astype(np.int16).tobytes(),\n            frame_rate=sr, sample_width=2, channels=1\n        )\n"
        },
        {
            "file_path": "harmonic.py",
            "content": "#File:  audio/harmonic.py \u00a9 2025 projectemergence. All rights reserved.\n# src/core/audio/harmonic.py\n\nimport math\nfrom typing import List, Tuple, Dict\n\nimport numpy as np\nfrom music21 import note, stream, analysis\n\n# build major/minor triad templates in pitch\u2010class space\n_NOTE_NAMES = ['C','C#','D','D#','E','F','F#','G','G#','A','A#','B']\n_CHORD_TEMPLATES: Dict[str, List[int]] = {}\nfor i, name in enumerate(_NOTE_NAMES):\n    _CHORD_TEMPLATES[f\"{name}\"]  = [i, (i+4)%12, (i+7)%12]   # major\n    _CHORD_TEMPLATES[f\"{name}m\"] = [i, (i+3)%12, (i+7)%12]   # minor\n\n\nclass HarmonicAnalyser:\n    \"\"\"\n    Real\u2010time key, chord & function estimation without external ML:\n      \u2022 key via music21.Krumhansl\n      \u2022 chords via 1\u2010beat triad\u2010template matching\n      \u2022 functions via scale\u2010degree rule (I/vi=T, ii/IV=S, V/vii\u00b0=D)\n    \"\"\"\n\n    def describe(\n        self,\n        melody: List[Tuple[float, float]]\n    ) -> Dict[str, List]:\n        \"\"\"\n        Input:\n          melody = [(frequency_Hz, duration_beats), \u2026]\n        Returns:\n          {\n            \"key\":       \"C major\",\n            \"chords\":    [...str chord symbol per beat\u2026],\n            \"functions\": [...int 0/1/2 per beat\u2026],\n            \"durations\": [...float, all 1.0\u2026]\n          }\n        \"\"\"\n        # 1) Key estimation\n        s = stream.Stream()\n        offset = 0.0\n        for freq, dur in melody:\n            n = note.Note()\n            n.pitch.frequency = freq\n            n.offset = offset\n            n.quarterLength = dur\n            s.insert(n)\n            offset += dur\n        key_obj = s.analyze(\"Krumhansl\")\n        key = f\"{key_obj.tonic.name} {key_obj.mode}\"\n\n        # 2) collect events as (time, pitch_class)\n        events: List[Tuple[float,int]] = []\n        t = 0.0\n        for freq, dur in melody:\n            midi = int(round(69 + 12*math.log2(freq/440.0)))\n            pc = midi % 12\n            events.append((t, pc))\n            t += dur\n\n        total_beats = math.ceil(offset)\n        chords:    List[str]   = []\n        durations: List[float] = []\n\n        tonic_pc = _NOTE_NAMES.index(key_obj.tonic.name)\n\n        # 3) for each beat\u2010window pick best\u2010matching triad\n        for b in range(total_beats):\n            window_pcs = [pc for time, pc in events if b <= time < b+1]\n            if not window_pcs:\n                # no notes \u2192 default to I\n                symbol = key_obj.tonic.name + (\"\" if key_obj.mode==\"major\" else \"m\")\n            else:\n                hist = np.zeros(12, dtype=int)\n                for pc in window_pcs:\n                    hist[pc] += 1\n                best_score, symbol = -1, key_obj.tonic.name\n                for sym, template in _CHORD_TEMPLATES.items():\n                    score = int(sum(hist[pc] for pc in template))\n                    if score > best_score:\n                        best_score, symbol = score, sym\n            chords.append(symbol)\n            durations.append(1.0)\n\n        # 4) map each chord \u2192 function (0=T,1=S,2=D)\n        functions: List[int] = []\n        for sym in chords:\n            root = sym.rstrip('m')\n            root_pc = _NOTE_NAMES.index(root) if root in _NOTE_NAMES else tonic_pc\n            interval = (root_pc - tonic_pc) % 12\n            if interval in (7, 11):      # V or vii\u00b0\n                functions.append(2)\n            elif interval in (2, 5):     # ii or IV\n                functions.append(1)\n            else:                        # I, vi, III, etc.\n                functions.append(0)\n\n        return {\n            \"key\":       key,\n            \"chords\":    chords,\n            \"functions\": functions,\n            \"durations\": durations\n        }\n"
        },
        {
            "file_path": "maestro_compositor.py",
            "content": "# File: src/core/audio/maestro_compositor.py \u00a9 2025 projectemergence. All rights reserved.\n\nimport os\nimport re\nimport json\nimport random\nfrom typing import List, Tuple, Dict, Optional\n\n# A single note event: (frequency in Hz, duration in beats, intensity 0\u20131)\nNoteEvent = Tuple[float, float, float]\n\nclass Compositor:\n    \"\"\"\n    \u2022 Loads JSON5\u2010style melody files (comments allowed)\n    \u2022 Captures all metadata (title, structure, remix params, etc.)\n    \u2022 Falls back to legacy \"notes\" arrays when \"hands\" is absent\n    \u2022 Exposes get_full_sequence() and next_event() as before\n    \"\"\"\n    def __init__(self, repo_path: str, default_intensity: float = 0.8):\n        self.repo_path         = repo_path\n        self.default_intensity = default_intensity\n\n        # name \u2192 metadata dict (everything except 'hands'/'notes')\n        self.metadata:  Dict[str, Dict]              = {}\n        # name \u2192 list of hands \u2192 list of NoteEvent\n        self.melodies:  Dict[str, List[List[NoteEvent]]] = {}\n        # name \u2192 tempo (BPM)\n        self.tempos:    Dict[str, float]              = {}\n        # name \u2192 (beats_per_bar, beat_unit)\n        self.meters:    Dict[str, Tuple[int,int]]     = {}\n\n        # playback state\n        self.current_hands:   List[List[NoteEvent]] = []\n        self.idxs:            List[int]             = []\n        self.current_melody:  Optional[str]         = None\n\n        print(f\"[Compositor] Scanning melodies in {self.repo_path}\")\n        self._load_repo()\n\n    def _load_repo(self):\n        def strip_comments(text: str) -> str:\n            # remove /* ... */ and //... comments\n            without_block = re.sub(r'/\\*.*?\\*/', '', text, flags=re.DOTALL)\n            without_line  = re.sub(r'//.*', '', without_block)\n            return without_line\n\n        for fn in sorted(os.listdir(self.repo_path)):\n            if not fn.lower().endswith(\".json\"):\n                continue\n            path = os.path.join(self.repo_path, fn)\n            name = fn[:-5]\n            try:\n                raw  = open(path, encoding=\"utf-8\").read()\n                data = json.loads(strip_comments(raw))\n            except Exception as e:\n                print(f\"[Compositor] Failed to parse {fn}: {e}\")\n                continue\n\n            # 1) store metadata (all keys except 'hands' and 'notes')\n            md = {k: v for k, v in data.items() if k not in (\"hands\", \"notes\")}\n            self.metadata[name] = md\n\n            # 2) read tempo & meter\n            if \"tempo\" in data:\n                self.tempos[name] = float(data[\"tempo\"])\n                print(f\"[Compositor] '{name}': tempo={data['tempo']} BPM\")\n            if \"time_signature\" in data:\n                tsig = data[\"time_signature\"]\n                try:\n                    num, den = map(int, tsig.split(\"/\"))\n                    self.meters[name] = (num, den)\n                    print(f\"[Compositor] '{name}': time_signature={tsig}\")\n                except:\n                    print(f\"[Compositor] '{name}': invalid time_signature '{tsig}'\")\n\n            # 3) gather raw hands or fallback to legacy notes\n            hands_raw = data.get(\"hands\")\n            if hands_raw is None and \"notes\" in data:\n                # legacy single-hand format\n                hands_raw = [data[\"notes\"]]\n\n            hands_evs: List[List[NoteEvent]] = []\n            if isinstance(hands_raw, list) and all(isinstance(h, list) for h in hands_raw):\n                for hl in hands_raw:\n                    evs: List[NoteEvent] = []\n                    for o in hl:\n                        try:\n                            f = float(o[\"frequency\"])\n                            d = float(o.get(\"duration_beats\", o.get(\"duration\", 1.0)))\n                            i = float(o.get(\"intensity\", self.default_intensity))\n                            evs.append((f, d, i))\n                        except:\n                            continue\n                    if evs:\n                        hands_evs.append(evs)\n\n            # 4) register or skip\n            if hands_evs:\n                self.melodies[name] = hands_evs\n                print(f\"[Compositor] Registered '{name}' with {len(hands_evs)} hand(s)\")\n            else:\n                print(f\"[Compositor] No valid events in '{name}', skipping\")\n\n    def start(self, melody_name: str):\n        \"\"\"Begin fresh run through up to 4 hands.\"\"\"\n        self.current_melody = melody_name\n        self.current_hands  = self.melodies.get(melody_name, [])\n        self.idxs           = [0] * len(self.current_hands)\n        print(f\"[Compositor] Starting '{melody_name}' with {len(self.current_hands)} hand(s)\")\n\n    def next_event(self) -> Tuple[List[float], List[float], List[float]]:\n        \"\"\"Return parallel lists: notes, durations, intensities.\"\"\"\n        if not self.current_hands:\n            return [0.0], [1.0], [0.0]\n\n        notes, durs, ints = [], [], []\n        for hi, hand in enumerate(self.current_hands):\n            f, d, i = hand[self.idxs[hi]]\n            notes.append(f)\n            durs.append(d)\n            ints.append(i)\n            self.idxs[hi] = (self.idxs[hi] + 1) % len(hand)\n\n        print(f\"[Compositor] next_event \u2192 notes={notes}, durs={durs}, ints={ints}\")\n        return notes, durs, ints\n\n    def get_full_sequence(self) -> Tuple[List[float], List[float], List[float]]:\n        \"\"\"\n        Return the entire upcoming sequence of notes, durations and intensities\n        for the *current* melody\u2019s first hand.\n        \"\"\"\n        if not self.current_hands:\n            return [], [], []\n        evs   = self.current_hands[0]\n        notes = [f for (f, _, _) in evs]\n        durs  = [d for (_, d, _) in evs]\n        ints  = [i for (_, _, i) in evs]\n        return notes, durs, ints\n\n    def sprinkle(self, chance: float = 0.1) -> bool:\n        hit = random.random() < chance\n        if hit:\n            print(f\"[Compositor] sprinkle fired (chance={chance})\")\n        return hit\n\n    def get_tempo(self, default: float) -> float:\n        \"\"\"Return the stored tempo for the current melody, or default.\"\"\"\n        return self.tempos.get(self.current_melody, default)\n\n    def get_meter(self) -> Tuple[int,int]:\n        \"\"\"Return (beats_per_bar, beat_unit) for current melody, default (4,4).\"\"\"\n        return self.meters.get(self.current_melody, (4,4))\n"
        },
        {
            "file_path": "maestro_mixer.py",
            "content": "#File:  audio/maestro_mixer.py \u00a9 2025 projectemergence. All rights reserved.\nimport numpy as np\nfrom collections import deque\n\nclass Mixer:\n    \"\"\"\n    Perform real-time spectral analysis and drive a parametric EQ.\n    \"\"\"\n    def __init__(self, client, sample_rate=48000, frame_size=1024):\n        self.client       = client\n        self.sr           = sample_rate\n        self.N            = frame_size\n        # rolling history of spectra for smoothing\n        self.history      = deque(maxlen=4)\n        # EQ band centers (Hz)\n        self.bands        = [125, 250, 500, 1000, 2000, 4000, 8000, 16000]\n        # initial gains\n        self.band_gains   = {b: 1.0 for b in self.bands}\n\n    def analyze_and_eq(self, audio_frame: np.ndarray):\n        \"\"\"\n        audio_frame: 1-D float32 buffer of length N.\n        \"\"\"\n        # 1) compute magnitude spectrum\n        spec = np.abs(np.fft.rfft(audio_frame * np.hanning(self.N)))\n        self.history.append(spec)\n        avg_spec = np.mean(self.history, axis=0)\n\n        # 2) for each band, find corresponding bin range\n        freqs = np.fft.rfftfreq(self.N, 1/self.sr)\n        for center in self.bands:\n            # find nearest bin\n            idx = np.argmin(np.abs(freqs - center))\n            magnitude = avg_spec[idx]\n            # map magnitude \u2192 desired gain (simple inverse)\n            # clamp between 0.5 and 1.0\n            gain = float(max(0.5, min(1.0, 1.0 - (magnitude / avg_spec.max())*0.5)))\n            # smooth update (one-pole)\n            self.band_gains[center] = 0.8*self.band_gains[center] + 0.2*gain\n            # push to client\u2019s EQ\n            self.client.set_eq_gain(center, self.band_gains[center])\n"
        },
        {
            "file_path": "orchestrator.py",
            "content": "#File:  audio/orchestrator.py \u00a9 2025 projectemergence. All rights reserved.\n# orchestrator.py\nfrom music21 import chord, instrument\n\nREGISTER = {\n    'bass': (28, 48),      # E1\u2013C3\n    'piano': (50, 96),     # D3\u2013C7\n    'pad': (40, 84),\n    'lead': (60, 108),\n}\n\nclass Orchestrator:\n    def __init__(self):\n        self.occ = {name: 0 for name in REGISTER}\n\n    def voice(self, chords: list[str], rhythm: list[float]):\n        parts = {}\n        for symb, dur in zip(chords, rhythm):\n            c = chord.Chord(symb)\n            bass_note = c.bass().pitch.midi\n            # allocate bass\n            parts.setdefault('bass', {'notes':[], 'durations':[], 'intensity':[]})\n            parts['bass']['notes'].append(self._fit(bass_note, 'bass'))\n            parts['bass']['durations'].append(dur)\n            parts['bass']['intensity'].append(.9)\n\n            # allocate chord tones to piano spread over two octaves\n            p_notes = [self._fit(n.pitch.midi, 'piano') for n in c.notes]\n            parts.setdefault('piano', {'notes':[], 'durations':[], 'intensity':[]})\n            parts['piano']['notes'].extend(p_notes)\n            parts['piano']['durations'].extend([dur/len(p_notes)]*len(p_notes))\n            parts['piano']['intensity'].extend([.7]*len(p_notes))\n        return parts\n\n    def _fit(self, midi, role):\n        low, high = REGISTER[role]\n        while midi < low:  midi += 12\n        while midi > high: midi -= 12\n        self.occ[role] += 1\n        return midi\n"
        },
        {
            "file_path": "progression.py",
            "content": "# File: src/core/audio/progression.py \u00a9 2025 projectemergence. All rights reserved.\n\nfrom typing import Dict, Any, List\nimport random\n\nfrom music21 import key as m21key, roman\n\nclass ProgressionSynth:\n    \"\"\"\n    Purely rule-based chord progression generator.\n    \u2022 Supports genre-specific templates (pop, rock, blues, jazz, classical, funk)\n    \u2022 Falls back to a functional-harmony Markov chain if needed\n    \u2022 Outputs real chord symbols in the current key (e.g. 'C', 'Am', 'F')\n    \"\"\"\n\n    # genre \u2192 Roman-numeral template per bar\n    GENRE_TEMPLATES: Dict[str, List[str]] = {\n        'pop':       ['I', 'V', 'vi', 'IV'],\n        'rock':      ['I', 'IV', 'V'],\n        'blues':     ['I', 'IV', 'I', 'V'],\n        'jazz':      ['ii', 'V', 'I'],\n        'classical': ['I', 'vi', 'ii', 'V'],\n        'funk':      ['I', 'bVII', 'IV', 'I'],\n    }\n\n    # Markov-chain transitions between functions: T\u2192S\u2192D\u2192T\n    FUNCTION_MARKOV: Dict[str, List[str]] = {\n        'T': ['S']*3 + ['D']*2 + ['T'],   # tonic tends to go to subdominant\n        'S': ['D']*4 + ['T'],             # subdominant to dominant\n        'D': ['T']*5 + ['S'],             # dominant resolves to tonic\n    }\n\n    # map Roman-numeral to its function\n    RN_TO_FUNCTION: Dict[str, str] = {\n        'I': 'T','i':'T','vi':'T','VI':'T','III':'T','iii':'T',\n        'ii':'S','II':'S','IV':'S','iv':'S',\n        'V':'D','v':'D','vii\u00b0':'D','VII':'D'\n    }\n\n    def __init__(self, genre: str = 'pop', temperature: float = 0.5):\n        \"\"\"\n        :param genre: one of GENRE_TEMPLATES keys; if unknown, uses Markov fallback\n        :param temperature: controls randomness in Markov chain (0\u20131)\n        \"\"\"\n        self.genre = genre if genre in self.GENRE_TEMPLATES else None\n        self.temperature = max(0.0, min(1.0, temperature))\n\n    def next(self, analysis: Dict[str,Any], beats: float) -> List[str]:\n        \"\"\"\n        :param analysis: must include 'key' (e.g. \"C major\") and optionally 'durations'\n        :param beats: total number of beats to fill\n        :returns: list of chord symbols (e.g. ['C','G','Am','F',\u2026])\n        \"\"\"\n        # parse the key\n        key_str = analysis.get('key', 'C major')\n        key_str = key_str.translate(str.maketrans({'\u266d':'b','\u266f':'#'}))\n        m21k = m21key.Key(key_str)\n\n        # estimate bars to fill (assume 4/4 unless analysis provides meter)\n        bar_beats = 4.0\n        # if 'time_signature' in analysis, override bar_beats\n        if 'time_signature' in analysis:\n            num, den = map(int, analysis['time_signature'].split('/'))\n            bar_beats = num * (4/den)\n        n_bars = max(1, int(round(beats / bar_beats)))\n\n        # choose progression template or fallback\n        if self.genre:\n            template = self.GENRE_TEMPLATES[self.genre]\n            # repeat/truncate to match n_bars\n            r = (template * ((n_bars // len(template)) + 1))[:n_bars]\n            rn_sequence = r\n        else:\n            # functional Markov\n            rn_sequence = []\n            prev_fn = 'T'\n            for _ in range(n_bars):\n                fn_choices = self.FUNCTION_MARKOV[prev_fn]\n                fn = random.choices(fn_choices, k=1)[0]\n                # pick a random Roman numeral with that function\n                cands = [rn for rn,f in self.RN_TO_FUNCTION.items() if f==fn]\n                rn = random.choice(cands)\n                rn_sequence.append(rn)\n                prev_fn = fn\n\n        # convert Roman numerals \u2192 concrete chord symbols\n        chords: List[str] = []\n        for rn in rn_sequence:\n            try:\n                rn_obj = roman.RomanNumeral(rn, m21k)\n                root = rn_obj.root().name  # e.g. 'C', 'A'\n                quality = rn_obj.quality  # 'major', 'minor', 'dominant', etc.\n            except Exception:\n                # fallback if music21 can't parse bVII etc.\n                # interpret 'bVII' relative to m21k\n                if rn.startswith('b'):\n                    degree = m21k.getScale().getDegreeFromPitch(rn[1:]) - 1\n                    root = m21k.pitchFromDegree(degree).name\n                    quality = 'major'\n                else:\n                    root = m21k.tonic.name\n                    quality = 'major'\n            # build symbol\n            if quality == 'minor':\n                sym = f\"{root}m\"\n            elif quality == 'dominant':\n                sym = f\"{root}7\"\n            else:\n                sym = root\n            chords.append(sym)\n\n        return chords\n"
        },
        {
            "file_path": "__init__.py",
            "content": "#File:  src/core/audio/__init__.py \u00a9 2025 projectemergence. All rights reserved.\n#File:  src/core/__init__.py \u00a9 2024 projectemergence. All rights reserved.\n# This file can be left empty, or you can use it to perform package-level initialization if needed.\n"
        }
    ]
}